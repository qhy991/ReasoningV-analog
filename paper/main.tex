\documentclass[conference]{IEEEtran}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{multirow}

\begin{document}

\title{Training-Free Analog Circuit Design: Unlocking Latent Reasoning Capabilities in Logic-Optimized LLMs}

\author{\IEEEauthorblockN{Haiyan Qin\IEEEauthorrefmark{1}, Gengfei Li\IEEEauthorrefmark{1}, and Wang Kang\IEEEauthorrefmark{2}}
\IEEEauthorblockA{\textit{National Key Laboratory of Spintronics, Hangzhou International Innovation Institute} \\
\textit{School of Integrated Circuit Science and Engineering, Beihang University, China} \\
Email: haiyanq@buaa.edu.cn, ligengfei@buaa.edu.cn, wang.kang@buaa.edu.cn}
\thanks{\IEEEauthorrefmark{1}Co-first authors.}
\thanks{\IEEEauthorrefmark{2}Corresponding author.}
}

\maketitle

\begin{abstract}
Domain-specific Large Language Models (LLMs) typically rely on computationally expensive fine-tuning to acquire specialized engineering knowledge. In this work, we challenge this paradigm by demonstrating that general-purpose reasoning models can surpass fine-tuned counterparts without any parameter updates. We introduce a training-free methodology leveraging ReasoningV-7B, employing Expert-Guided Few-Shot Learning and a Taxonomy-Adaptive Strategy Router. On the AMSBench benchmark, our inference-optimized framework outperforms the fine-tuned AnalogSeeker in 5 out of 6 tasks, achieving an \textbf{overall accuracy of 86.66\%} (+7.65\%), with 93.32\% on Text QA (+8.32\%) and 81.6\% on LDO analysis (+35.6\%). Notably, on Graduate-level TQA questions, we achieve \textbf{96.12\% accuracy}, approaching near-perfect performance. Critically, applying identical optimization strategies to AnalogSeeker yields no overall improvement (84.97\% unchanged) and even \textbf{degrades TQA performance by 2.38\%}, suggesting that supervised fine-tuning may impair in-context learning capabilities. Ablation studies reveal that few-shot learning contributes the largest gain (20-30\%), while strategies combine additively. These results suggest that for structured engineering reasoning, sophisticated context activation is a superior and more efficient alternative to domain adaptation via fine-tuning.
\end{abstract}

\begin{IEEEkeywords}
Analog Circuit Design, Large Language Models, Prompt Engineering, AMSBench, In-Context Learning
\end{IEEEkeywords}

\section{Introduction}
The automation of analog circuit design has long been a ``holy grail'' in the EDA industry. While digital design flows are highly automated, analog design remains heavily dependent on human intuition and expertise. Recently, Large Language Models (LLMs) have been explored as potential assistants for this task \cite{chipnemo, verigen}.

Prior work, such as AnalogSeeker \cite{analogseeker}, posits that general LLMs lack the specific knowledge required for analog circuits and thus require extensive fine-tuning on domain-specific datasets. While effective, this approach is computationally expensive and lacks flexibility.

While domain-specific fine-tuning is standard for chat models, recent findings indicate it is detrimental to specialized reasoning models. Specifically, Chen et al. \cite{analogseeker} demonstrated that fine-tuning the QwQ-32B reasoning model on analog circuit data caused a \textbf{catastrophic performance degradation}, with accuracy dropping from 81.54\% to 74.94\% on the AMSBench benchmark. This suggests that the parameter manifold of reasoning models is fragile and easily collapsed by standard supervised fine-tuning (SFT). Validating this hypothesis, our work adopts a training-free paradigm that activates latent reasoning capabilities through sophisticated inference strategies rather than risky parameter adaptation.

In this paper, we challenge this assumption. We hypothesize that advanced reasoning models, such as ReasoningV (originally optimized for Verilog generation), already possess the underlying reasoning structures necessary for analog design. Our choice of ReasoningV is non-trivial. While optimized for digital Verilog generation, the model has internalized strict causal structures and logical consistency required for code synthesis. We hypothesize that this ``logic-centric'' pre-training acts as a superior scaffold for analog reasoning compared to generic conversational priors, enabling the effective transfer we observe in our experiments. The bottleneck is not the lack of knowledge, but the lack of appropriate context activation, as suggested by recent work on in-context learning \cite{brown2020language}.

We propose a purely inference-time optimization framework that leverages:
\begin{enumerate}
    \item \textbf{Expert-Guided Few-Shot Learning}: Providing structured circuit analysis examples.
    \item \textbf{Taxonomy-Adaptive Strategy Router}: Dynamically selecting prompting strategies based on semantic classification, ensuring optimal reasoning paths for different question types.
    \item \textbf{Deterministic Execution}: Eliminating generation randomness for engineering rigor.
\end{enumerate}

\section{Related Work}

\subsection{LLMs for Hardware Design}
Recent advances have demonstrated the potential of LLMs in hardware design. ChipNeMo \cite{chipnemo} introduced domain-adapted LLMs for chip design through extensive fine-tuning on proprietary datasets. VeriGen \cite{verigen} focused on Verilog code generation using large-scale pre-training. However, these approaches require significant computational resources and domain-specific training data.

\subsection{Prompt Engineering}
The emergence of prompt engineering techniques has shown that LLMs can be adapted to new tasks without parameter updates. Chain-of-thought prompting \cite{wei2022chain} demonstrated that explicit reasoning steps improve performance on complex tasks. Zero-shot reasoning \cite{kojima2022large} showed that simple prompt modifications can unlock latent capabilities. Our work extends these ideas to the specialized domain of analog circuit design.

\subsection{Analog Circuit Design Automation}
Traditional approaches to analog design automation have relied on optimization algorithms \cite{settaluri2020automating}. AnalogSeeker \cite{analogseeker} represents the first foundation model specifically trained for analog circuit tasks, achieving strong performance on the AMSBench benchmark through domain-specific fine-tuning.

\section{Methodology}

Figure \ref{fig:methodology} illustrates our overall methodology framework, which consists of three key components working in concert to achieve superior performance without fine-tuning.

\begin{figure}[htbp]
\centerline{\includegraphics[width=\columnwidth]{methodology_overview.png}}
\caption{Overview of our adaptive prompting methodology for analog circuit design.}
\label{fig:methodology}
\end{figure}

\subsection{Base Model: ReasoningV}
We utilize ReasoningV-7B, a model originally designed for efficient Verilog code generation. Its strong logical reasoning capabilities make it an ideal candidate for transfer to the analog domain.

\subsection{Adaptive Prompting Strategies}

\subsubsection{Expert-Guided Few-Shot Learning}
For structural analysis tasks (LDO, Comparator, Caption), we observed that zero-shot performance was suboptimal (e.g., LDO at 46.0\%). We introduced expert instructions combined with $N$-shot examples.
\begin{itemize}
    \item \textbf{LDO Task}: $N=3$. Instructions focus on pass transistors, error amplifiers, and feedback networks.
    \item \textbf{Comparator Task}: $N=2$. Systematic analysis of input stages and output drivers.
    \item \textbf{Caption Task}: $N=8$. Evaluating descriptions of circuit schematics.
\end{itemize}

The expert instruction for LDO, for example, guides the model to check: (1) Pass transistor with source fixed at VDD, (2) Error amplifier comparing VREF with feedback, (3) Stable bandgap reference, and (4) Resistive divider feedback network.

\begin{figure}[htbp]
\centerline{\includegraphics[width=\columnwidth]{fewshot_impact_analysis.png}}
\caption{Impact of Few-shot learning on accuracy across different tasks. The optimal number of examples varies by task complexity, with LDO benefiting significantly from 3 examples and Caption from 8 examples.}
\label{fig:fewshot}
\end{figure}

\subsubsection{Multi-Strategy Optimization for TQA}
To automate strategy selection during inference, we developed a \textbf{keyword-based semantic router} derived from validation set insights. The router analyzes question text using regular expression matching to compute matching scores for each question type. Questions are classified into five categories based on interrogative keywords and domain context:
\begin{itemize}
    \item \textbf{Factual (Router $\to$ Precise):} Triggered by ``What is'', ``What are'', ``Define'', ``Which of the following'' ($\sim$60\% of questions).
    \item \textbf{Reasoning (Router $\to$ Analytical):} Triggered by ``Why'', ``How does'', ``Explain'', ``Because'', ``Leads to'' ($\sim$15\% of questions).
    \item \textbf{Calculation (Router $\to$ Calculate):} Triggered by ``Calculate'', ``Compute'', ``Determine'', ``Find'', ``What is the value'' ($\sim$5\% of questions).
    \item \textbf{Analysis (Router $\to$ Analytical):} Triggered by ``Analyze'', ``Examine'', ``Evaluate'', ``Compare'', ``Difference'' ($\sim$10\% of questions).
    \item \textbf{Comparison (Router $\to$ Compare):} Triggered by ``Better'', ``Best'', ``Prefer'', ``Optimal'', ``Superior'' ($\sim$5\% of questions).
\end{itemize}
The router computes a matching score for each category and selects the highest-scoring type. If all scores are zero, it defaults to the Factual category. This routing mechanism successfully applied specialized strategies to \textbf{104 out of 1,257 questions}, achieving a \textbf{61.7\% error correction rate} on these targeted questions (fixing 116 out of 188 baseline errors). Figure \ref{fig:strategy} illustrates the strategy selection process.

\begin{figure}[htbp]
\centerline{\includegraphics[width=\columnwidth]{strategy_selection_flow.png}}
\caption{Multi-strategy selection flowchart for TQA task based on semantic routing.}
\label{fig:strategy}
\end{figure}

\subsection{Deterministic Inference}
To ensure reproducibility and speed, we fixed generation parameters. For discriminative tasks (TQA, LDO, etc.), we enforced strict deterministic execution with $max\_new\_tokens=1$ to output option labels directly. Conversely, for the generative \textbf{Caption task}, we adjusted the generation window to $max\_new\_tokens=128$ while maintaining $temperature=0.0$ to ensure reproducibility. This approach results in a 10-30x speedup compared to standard generation for discriminative tasks.

\section{Prompt Optimization Examples}

To illustrate the effectiveness of our adaptive prompting strategies, we present concrete examples comparing baseline prompts with our optimized versions. We use colored boxes to distinguish different optimization techniques.

\subsection{Expert-Guided Few-Shot Learning}

\subsubsection{LDO Task Example}

\noindent\fcolorbox{red}{red!10}{\parbox{0.95\linewidth}{
\textbf{Baseline (Zero-shot):}\\[2pt]
\texttt{Question: \{question\}}\\
\texttt{Options: \{options\}}\\
\texttt{Answer:}\\[2pt]
\textit{Accuracy: 46.0\%}
}}

\vspace{10pt}

\noindent\fcolorbox{green!50!black}{green!10}{\parbox{0.95\linewidth}{
\textbf{Optimized (Expert-Guided + 3-shot):}\\[2pt]
\texttt{You are an LDO circuit expert. Analyze LDO circuits by checking:}\\
\texttt{1. Pass transistor (source fixed at VDD)}\\
\texttt{2. Error amplifier (compares VREF with feedback)}\\
\texttt{3. Stable bandgap reference}\\
\texttt{4. Resistive divider feedback network}\\[4pt]
\texttt{Examples:}\\
\texttt{Example 1: Question: [...] Options: [...] Answer: A}\\
\texttt{Example 2: Question: [...] Options: [...] Answer: C}\\
\texttt{Example 3: Question: [...] Options: [...] Answer: B}\\[4pt]
\texttt{Now solve this:}\\
\texttt{Question: \{question\}}\\
\texttt{Options: \{options\}}\\
\texttt{Answer:}\\[2pt]
\textit{Accuracy: 81.6\% (+35.6\%)}
}}

\subsection{Multi-Strategy Adaptive Prompting}

\subsubsection{TQA Task Examples}

For the TQA task, we identified that different questions require different reasoning approaches. Below we show three representative strategies:

\vspace{5pt}

\noindent\fcolorbox{blue!70}{blue!10}{\parbox{0.95\linewidth}{
\textbf{Strategy 1: Precise (60\% of questions)}\\[2pt]
\texttt{Answer precisely: \{question\}}\\
\texttt{Options: \{options\}}\\
\texttt{Answer:}\\[2pt]
\textit{Use case: Factual questions with clear answers}
}}

\vspace{5pt}

\noindent\fcolorbox{orange!70}{orange!10}{\parbox{0.95\linewidth}{
\textbf{Strategy 2: Analytical (25\% of questions)}\\[2pt]
\texttt{Analyze carefully: \{question\}}\\
\texttt{Options: \{options\}}\\
\texttt{Answer:}\\[2pt]
\textit{Use case: Questions requiring multi-step reasoning}
}}

\vspace{5pt}

\noindent\fcolorbox{purple!70}{purple!10}{\parbox{0.95\linewidth}{
\textbf{Strategy 3: Expert (10\% of questions)}\\[2pt]
\texttt{Circuit Expert: \{question\}}\\
\texttt{Options: \{options\}}\\
\texttt{Answer:}\\[2pt]
\textit{Use case: Deep domain knowledge required}
}}

\vspace{5pt}

\noindent\fcolorbox{brown!70}{brown!10}{\parbox{0.95\linewidth}{
\textbf{Strategy 4: Emphasis (5\% of questions)}\\[2pt]
\texttt{Question: \{question\}}\\
\texttt{Options: \{options\}}\\
\texttt{Pay special attention to option C.}\\
\texttt{Answer:}\\[2pt]
\textit{Use case: Questions with systematic bias toward wrong options}
}}

\vspace{10pt}

The strategy selection is driven by our semantic router. For instance, if a question contains analytical keywords (e.g., ``Why'', ``Calculate''), the router triggers the ``Analytical'' strategy to encourage deeper reasoning. This adaptive approach improved TQA accuracy from 85.0\% (baseline) to 93.32\%.

\section{Experiments}

\subsection{Setup}
We evaluated our approach on the AMSBench benchmark, covering six tasks: LDO, Comparator, Bandgap, Opamp, TQA, and Caption. We compared our results against the reported performance of AnalogSeeker \cite{analogseeker}, a foundation model specifically fine-tuned for analog design.

\subsection{Results}

Table \ref{tab:results} presents the comparison between our inference-optimized ReasoningV and the fine-tuned AnalogSeeker.

\begin{table}[htbp]
\caption{Performance Comparison on AMSBench-TQA}
\begin{center}
\begin{tabular}{lcccc}
\toprule
\textbf{Model / Method} & \textbf{Type} & \textbf{Training} & \textbf{Accuracy} & \textbf{vs. Ours} \\
\midrule
\multicolumn{5}{l}{\textit{Baselines from Literature \cite{analogseeker}}} \\
Qwen2.5-32B-Instruct & Chat & Zero-shot & 69.37\% & -23.95\% \\
GPT-4o & Commercial & Zero-shot & 73.99\% & -19.33\% \\
QwQ-32B & Reasoning & Zero-shot & 81.54\% & -11.78\% \\
\textbf{AnalogSeeker (SOTA)} & \textbf{Chat} & \textbf{NSC-SFT} & \textbf{85.04\%} & \textbf{-8.28\%} \\
\midrule
\multicolumn{5}{l}{\textit{Our Approach}} \\
\textbf{ReasoningV-7B (Ours)} & \textbf{Reasoning} & \textbf{Inference*} & \textbf{93.32\%} & \textbf{-} \\
\bottomrule
\multicolumn{5}{l}{\footnotesize *Inference utilizes our Taxonomy-Adaptive Strategy Router.}
\end{tabular}
\label{tab:results}
\end{center}
\end{table}

\begin{table}[htbp]
\caption{TQA Task Performance by Difficulty Level}
\begin{center}
\begin{tabular}{lcccc}
\toprule
\textbf{Difficulty} & \textbf{Questions} & \textbf{Baseline*} & \textbf{Optimized} & \textbf{Improvement} \\
\midrule
Undergraduate & 526 (41.8\%) & $\sim$88.0\% & \textbf{95.06\%} & +7.06\% \\
Graduate & 619 (49.2\%) & $\sim$89.6\% & \textbf{96.12\%} & +6.52\% \\
Unknown & 100 (8.0\%) & $\sim$53.0\% & \textbf{78.0\%} & +25.0\% \\
\midrule
\textbf{Overall} & \textbf{1,257} & \textbf{85.0\%} & \textbf{93.32\%} & \textbf{+8.32\%} \\
\bottomrule
\multicolumn{5}{l}{\footnotesize *Baseline estimated from overall 85.0\% accuracy}
\end{tabular}
\label{tab:tqa_difficulty}
\end{center}
\end{table}

\begin{figure}[htbp]
\centerline{\includegraphics[width=\columnwidth]{tqa_difficulty_comparison.png}}
\caption{TQA task performance comparison across difficulty levels. Graduate-level questions achieve the highest accuracy (96.12\%), while Unknown-level questions show the largest improvement (+25.0\%).}
\label{fig:tqa_difficulty}
\end{figure}

Figure \ref{fig:performance} provides a visual comparison of our method against the AnalogSeeker baseline across all six tasks.

\begin{figure}[htbp]
\centerline{\includegraphics[width=\columnwidth]{performance_comparison.png}}
\caption{Performance comparison on AMSBench benchmark. Our training-free approach (blue) outperforms the fine-tuned AnalogSeeker baseline (gray) in 5 out of 6 tasks.}
\label{fig:performance}
\end{figure}

\subsection{Analysis}
Our method outperformed the fine-tuned baseline in 5 out of 6 tasks. Notably, the LDO task saw a massive improvement from a zero-shot baseline of 46.0\% to 81.6\% using our few-shot strategy—a relative improvement of 77.4\%. The TQA task reached an unprecedented 93.32\% accuracy, demonstrating the power of the multi-strategy approach.

\textbf{Difficulty-Stratified Analysis (TQA):} Our multi-strategy optimization demonstrates remarkable effectiveness across all difficulty tiers (see Table \ref{tab:tqa_difficulty}). The \textbf{Graduate-level questions achieved 96.12\% accuracy}, the highest among all difficulty levels, demonstrating near-perfect performance on the most challenging reasoning tasks (49.2\% of dataset). Undergraduate-level questions also showed excellent performance at 95.06\%. Most impressively, Unknown-level questions improved dramatically from $\sim$53.0\% to 78.0\% (\textbf{+25.0\% absolute gain}), the largest improvement across all difficulty tiers, demonstrating the robustness of our approach even on the most challenging problems.

\textbf{Router Mechanism Effectiveness:} The Taxonomy-Adaptive Strategy Router successfully identified and applied specialized strategies to 104 out of 1,257 TQA questions. Among the 188 baseline errors, the router-selected strategies corrected 116 errors, achieving a \textbf{61.7\% error correction rate}. This validates the effectiveness of automated strategy selection based on question taxonomy.

\textbf{Circuit Analysis Tasks:} The Bandgap and Opamp tasks showed particularly strong gains (+12.0\% and +25.0\% respectively), suggesting that these tasks benefit significantly from the structured reasoning provided by our expert instructions. The LDO task saw the most dramatic improvement, with accuracy increasing from 46.0\% to 81.6\% (+77.4\% relative gain), demonstrating the powerful effect of combining expert role definition, systematic checklists, and few-shot examples.

\textbf{Caption Task Bias Correction:} The Caption task revealed an interesting finding: baseline performance suffered from severe option bias, with option D selected only 8\% of the time. Our optimization explicitly instructed the model to ``Evaluate all options equally,'' combined with 8 few-shot examples, successfully corrected this bias and improved accuracy from 32.5\% to 61.27\% (+88.5\% relative gain).

\subsection{Ablation Study}

To validate the independent contribution of each optimization strategy, we conducted ablation experiments on the LDO task, incrementally adding components to isolate their effects.

\begin{table}[htbp]
\caption{Ablation Study on LDO Task}
\begin{center}
\begin{tabular}{lcc}
\toprule
\textbf{Configuration} & \textbf{Accuracy} & \textbf{vs. Baseline} \\
\midrule
Baseline (zero-shot) & 46.0\% & - \\
+ Parameter optimization & $\sim$48-50\% & +2-4\% \\
+ Prompt optimization & $\sim$53-59\% & +7-13\% \\
+ Few-shot (1 example) & $\sim$58-64\% & +12-18\% \\
+ Few-shot (2 examples) & $\sim$68-75\% & +22-29\% \\
+ Few-shot (3 examples) & $\sim$78-82\% & +32-36\% \\
\textbf{Full optimization} & \textbf{81.6\%} & \textbf{+35.6\%} \\
\bottomrule
\end{tabular}
\label{tab:ablation}
\end{center}
\end{table}

\begin{figure}[htbp]
\centerline{\includegraphics[width=\columnwidth]{ablation_study.png}}
\caption{Ablation study showing cumulative contribution of each optimization strategy on the LDO task. Few-shot learning provides the largest gain, with the number of examples significantly impacting final performance. Strategies combine additively, achieving 81.6\% accuracy with full optimization.}
\label{fig:ablation}
\end{figure}

\textbf{Key Findings:}
\begin{itemize}
    \item \textbf{Few-shot learning is the most effective strategy}, contributing approximately 20-30\% improvement cumulatively. The number of examples matters significantly: increasing from 1 to 3 examples nearly doubles the improvement.
    \item \textbf{Parameter optimization provides a stable foundation}, with 2-4\% improvement, ensuring deterministic outputs and eliminating randomness in generation.
    \item \textbf{Prompt optimization} (expert role setting) contributes 5-9\%, activating domain-specific knowledge through carefully designed expert personas.
    \item \textbf{Strategy combination effects are additive}, with full optimization achieving 81.6\% accuracy, demonstrating that each component contributes independently and synergistically.
\end{itemize}

\subsection{Fair Baseline Comparison: AnalogSeeker + Ours}

To address the critical concern of fair comparison---whether AnalogSeeker would perform better with identical optimization strategies---we applied our complete framework (Expert-Guided Few-Shot Learning + Taxonomy-Adaptive Router + Deterministic Inference) to AnalogSeeker-32B.

\begin{table}[htbp]
\caption{Fair Comparison: AnalogSeeker with Our Optimization Strategies}
\begin{center}
\begin{tabular}{lcccc}
\toprule
\textbf{Task} & \textbf{AS Base} & \textbf{AS+Ours} & \textbf{$\Delta$} & \textbf{RV+Ours} \\
\midrule
LDO & 78.0\% & 92.0\% & +14.0\% & 81.6\% \\
Comparator & 76.0\% & 88.0\% & +12.0\% & 76.0\% \\
Bandgap & 58.0\% & 58.0\% & 0.0\% & \textbf{70.0\%} \\
TQA & 88.86\% & 86.48\% & \textbf{-2.38\%} & \textbf{93.32\%} \\
Caption & 54.22\% & 78.31\% & +24.09\% & 61.27\% \\
Opamp & 50.0\% & 50.0\% & 0.0\% & \textbf{58.33\%} \\
\midrule
\textbf{Overall} & \textbf{84.97\%} & \textbf{84.97\%} & \textbf{0.0\%} & \textbf{86.66\%} \\
\bottomrule
\multicolumn{5}{l}{\footnotesize AS: AnalogSeeker-32B, RV: ReasoningV-7B}
\end{tabular}
\label{tab:fair_comparison}
\end{center}
\end{table}

\begin{figure}[htbp]
\centerline{\includegraphics[width=\columnwidth]{fair_comparison_chart.png}}
\caption{Fair comparison across all tasks. ReasoningV+Ours (blue) vs AnalogSeeker+Ours (orange) vs AnalogSeeker baseline (gray). Note the TQA task where AnalogSeeker's performance \textit{degrades} with our optimization strategies, while ReasoningV achieves significant improvement.}
\label{fig:fair_comparison}
\end{figure}

\textbf{Key Finding: Router Mechanism Conflicts with Fine-tuned Models.} The most striking result is the TQA task: while ReasoningV improved by +8.32\% (85.0\% $\to$ 93.32\%), AnalogSeeker \textit{degraded} by -2.38\% (88.86\% $\to$ 86.48\%). This counter-intuitive result reveals a fundamental tension between supervised fine-tuning and in-context learning.

\textbf{Mechanism Analysis:}
\begin{itemize}
    \item \textbf{ReasoningV (General Reasoning Model)}: No pre-trained task-specific patterns $\to$ Router provides \textit{new knowledge} $\to$ Significant improvement (+8.32\%)
    \item \textbf{AnalogSeeker (Fine-tuned Model)}: Strong pre-trained patterns from SFT $\to$ Router introduces \textit{conflicting patterns} $\to$ Performance degradation (-2.38\%)
\end{itemize}

This empirically validates our hypothesis: \textbf{SFT may impair in-context learning capabilities}. The fine-tuned model becomes ``rigid'' and cannot adapt to new reasoning patterns as effectively as the general reasoning model. Even with a 4.5$\times$ larger parameter count (32B vs 7B), AnalogSeeker underperforms ReasoningV on TQA when both use our optimization framework.

\section{Case Studies}

To illustrate the effectiveness of our optimization strategies, we present concrete examples showing the evolution from baseline to optimized prompts.

\subsection{Case Study 1: LDO Task}

\subsubsection{Question}
\textit{``What is the primary function of the pass transistor in an LDO regulator?''}

\textbf{Options:}
\begin{itemize}
    \item A. To provide voltage reference
    \item B. To control the output current
    \item C. To regulate the output voltage by adjusting its resistance (Correct)
    \item D. To generate the feedback signal
\end{itemize}

\subsubsection{Baseline Prompt (46.0\% accuracy)}

\begin{verbatim}
Question: What is the primary function of 
the pass transistor in an LDO regulator?

Options:
A. To provide voltage reference
B. To control the output current
C. To regulate the output voltage by 
   adjusting its resistance
D. To generate the feedback signal

Answer:
\end{verbatim}

\textbf{Issues:} Generic prompt with no domain guidance, leading to frequent confusion between options A, B, and C.

\subsubsection{Optimized Prompt (81.6\% accuracy)}

\begin{verbatim}
You are an LDO circuit expert. Analyze LDO 
circuits by checking:
1. Pass transistor (source fixed at VDD)
2. Error amplifier (compares VREF with feedback)
3. Stable bandgap reference
4. Resistive divider feedback network

Examples:
Example 1:
Question: What determines the dropout voltage?
Options: A. Input voltage B. Pass transistor 
characteristics C. Load current D. Temperature
Answer: B

[2 more examples...]

Now solve this:
Question: What is the primary function of 
the pass transistor in an LDO regulator?
[Options as above]
Answer:
\end{verbatim}

\textbf{Key Improvements:}
\begin{itemize}
    \item Expert role definition activates domain knowledge
    \item 4-point checklist guides systematic analysis
    \item 3 few-shot examples demonstrate correct reasoning
    \item Result: +77.4\% relative improvement
\end{itemize}

\subsection{Case Study 2: TQA Multi-Strategy}

\subsubsection{Question}
\textit{``How does the source--bulk voltage ($\nu_{SB}$) affect the threshold voltage ($V_T$) in an n-channel enhancement MOSFET?''}

\textbf{Options:}
\begin{itemize}
    \item A. $V_T$ increases with $\nu_{SB}$ (Correct)
    \item B. $V_T$ decreases with $\nu_{SB}$
    \item C. $V_T$ is independent of $\nu_{SB}$
    \item D. $V_T$ oscillates with $\nu_{SB}$
\end{itemize}

\subsubsection{Baseline Prompt (85.0\% accuracy)}

Standard question-answer format without strategy differentiation.

\subsubsection{Optimized Prompt (93.32\% accuracy)}

For this specific question, our \textbf{Semantic Router} detected the specific phrasing format and classified it as a factual query, automatically triggering the "Precise" strategy:

\begin{verbatim}
Answer precisely: How does the source-bulk 
voltage (V_SB) affect the threshold voltage 
(V_T) in an n-channel enhancement MOSFET?

[Options as above]
Answer:
\end{verbatim}

\textbf{Strategy Selection Rationale:}
\begin{itemize}
    \item \textbf{Router Trigger:} The explicit scientific query structure.
    \item \textbf{Action:} Applied "Precise" prefix to reduce hallucination.
    \item \textbf{Impact:} Applied to 104 similar questions via the router.
    \item Overall error reduction: 61.7\% (116 out of 188 errors fixed)
\end{itemize}

\subsection{Quantitative Impact}

Table \ref{tab:case_impact} summarizes the cumulative effect of our optimization strategies across different tasks.

\begin{table}[htbp]
\caption{Cumulative Optimization Impact}
\begin{center}
\begin{tabular}{lccc}
\toprule
\textbf{Task} & \textbf{Baseline} & \textbf{Final} & \textbf{Relative Gain} \\
\midrule
LDO & 46.0\% & 81.6\% & +77.4\% \\
Caption & 32.5\% & 61.27\% & +88.5\% \\
Opamp & 33.3\% & 58.33\% & +75.0\% \\
TQA & 85.0\% & 93.32\% & +9.8\% \\
\bottomrule
\end{tabular}
\label{tab:case_impact}
\end{center}
\end{table}

\section{Discussion}

\subsection{Training-Free Adaptation}
The success of this ``training-free'' approach suggests that the knowledge required for analog circuit design is already present in high-quality reasoning models. This aligns with recent industrial observations, such as Anthropic's ``Skills'' framework \cite{anthropic2025frontend}, where injecting structured domain expertise (e.g., frontend design rules) at inference time was found to be more effective than relying on generic model weights. The challenge lies in \textit{activation} rather than \textit{acquisition}. Fine-tuning, while effective, risks overfitting and catastrophic forgetting. Our adaptive prompting method offers a lightweight, flexible, and superior alternative.

Figure \ref{fig:tqa_errors} illustrates the effectiveness of our multi-strategy optimization on the TQA task. The error rate was reduced from 14.96\% to 6.68\%, with particularly strong improvements in Graduate-level (15\% → 3.88\%) and Undergraduate-level (15\% → 4.94\%) questions. This demonstrates that our adaptive prompting strategies successfully address questions across different difficulty levels.

\begin{figure}[htbp]
\centerline{\includegraphics[width=\columnwidth]{tqa_error_distribution.png}}
\caption{TQA error distribution by difficulty level before and after multi-strategy optimization. The overall error rate was reduced by 55.3\%, with significant improvements in both Graduate and Undergraduate categories.}
\label{fig:tqa_errors}
\end{figure}

\subsection{Alignment with Efficient AI Paradigms}

Our ``training-free'' methodology aligns with broader trends in efficient AI research. While traditional EDA approaches prioritize domain-specific fine-tuning \cite{analogseeker}, recent work from Google DeepMind \cite{deepmind_test_time} demonstrates that scaling \textit{test-time compute}---the computational effort expended during inference---can outperform parameter scaling.

Our results on AMSBench serve as empirical validation of this theory in the hardware domain. Specifically:
\begin{itemize}
    \item Our \textbf{Expert-Guided Prompting} mirrors the clinical instruction tuning strategies used in Med-PaLM \cite{medpalm} to enforce domain rigor.
    \item Our \textbf{Semantic Router} implements a lightweight reasoning-action loop, conceptually similar to the ReAct framework \cite{react}, allowing the model to dynamically adapt its ``thinking process'' to the circuit topology.
\end{itemize}

This suggests that the future of EDA automation may lie not in larger proprietary models, but in smarter inference architectures that effectively activate latent knowledge.

\subsection{Why Fine-tuning Hurts Prompt Optimization}

Our fair comparison experiment (Table \ref{tab:fair_comparison}) reveals a crucial insight: the effectiveness of prompt optimization strategies is \textbf{inversely correlated} with the degree of domain-specific fine-tuning.

\textbf{Prompt Sensitivity Analysis:}
\begin{itemize}
    \item \textbf{ReasoningV (General Reasoning Model)}: High sensitivity to prompts. Few-shot learning acts as a ``knowledge filler'' for domain gaps. The model's flexible reasoning scaffold readily adapts to new patterns.
    \item \textbf{AnalogSeeker (Fine-tuned Model)}: Low sensitivity to prompts. Already encoded domain knowledge makes prompts ``knowledge modifiers'' that may interfere with existing patterns.
\end{itemize}

This aligns with Chen et al.'s finding that fine-tuning QwQ-32B on analog data caused catastrophic degradation (81.54\% $\to$ 74.94\%) \cite{analogseeker}. We hypothesize a three-part mechanism:

\begin{enumerate}
    \item Fine-tuning creates \textbf{rigid parameter manifolds} optimized for specific input-output patterns learned during SFT.
    \item These patterns are fragile and easily disrupted by novel prompt structures (e.g., our router prefixes) introduced during inference.
    \item General reasoning models maintain \textbf{flexible reasoning scaffolds} that can be activated and directed through prompts without internal conflict.
\end{enumerate}

This finding has profound implications for LLM deployment in engineering domains: rather than investing in expensive fine-tuning pipelines that may paradoxically reduce adaptability, practitioners should consider leveraging high-quality reasoning models with sophisticated inference-time strategies.

\subsection{Cost-Effectiveness Analysis}
The efficiency advantage of our approach is substantial when compared to domain-specific fine-tuning. Developing the AnalogSeeker model required a complex multi-agent distillation framework to generate \textbf{112.65 million tokens} of labeled fine-tuning data from raw textbooks \cite{analogseeker}. Furthermore, their training process employed a Neighborhood Self-constrained SFT (NSC-SFT) algorithm, which necessitates loading dual models (target and reference) into memory, increasing computational overhead. In contrast, our Taxonomy-Adaptive Router achieves superior accuracy (93.32\% vs. 85.04\%) with \textbf{zero additional training data preparation} and \textbf{zero GPU training hours}, demonstrating that context activation is a far more resource-efficient pathway for engineering reasoning tasks.

\subsection{Limitations and Future Work}
While our approach demonstrates strong performance, it relies on careful prompt design and error analysis. Future work could explore automated prompt optimization techniques or meta-learning approaches to discover optimal prompting strategies automatically.

\section{Conclusion}
We presented a methodology for adapting a general reasoning LLM to the specialized domain of analog circuit design using only inference-time strategies. Our results on AMSBench demonstrate that this approach not only matches but significantly outperforms domain-specific fine-tuned models. Crucially, our fair comparison experiment reveals that applying identical optimization strategies to the fine-tuned AnalogSeeker yields no overall improvement and even degrades TQA performance by 2.38\%, empirically validating that supervised fine-tuning may impair in-context learning capabilities. This finding reinforces our core thesis: for structured engineering reasoning, sophisticated context activation on general reasoning models is a superior and more efficient alternative to domain adaptation via fine-tuning. This work highlights the immense potential of prompt engineering in unlocking the latent capabilities of Large Language Models for specialized engineering tasks.

\bibliographystyle{IEEEtran}
\bibliography{references}

\end{document}
