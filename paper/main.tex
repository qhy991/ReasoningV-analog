\documentclass[conference]{IEEEtran}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{multirow}

\begin{document}

\title{Training-Free Analog Circuit Design: Unlocking Latent Reasoning Capabilities in Logic-Optimized LLMs}

\author{\IEEEauthorblockN{Gengfei Li}
\IEEEauthorblockA{\textit{Dept. of Microelectronics} \\
\textit{Shanghai Jiao Tong University}\\
Shanghai, China}
}

\maketitle

\begin{abstract}
Domain-specific Large Language Models (LLMs) typically rely on computationally expensive fine-tuning to acquire specialized engineering knowledge. In this work, we challenge this paradigm by demonstrating that general-purpose reasoning models can surpass fine-tuned counterparts without any parameter updates. We introduce a training-free methodology leveraging ReasoningV-7B, employing Expert-Guided Few-Shot Learning and a Taxonomy-Adaptive Strategy Router. On the AMSBench benchmark, our inference-optimized framework outperforms the fine-tuned AnalogSeeker in 5 out of 6 tasks, achieving a 93.32\% accuracy in Text QA (+8.32\%) and an 81.6\% accuracy in LDO analysis (+3.6\%). These results suggest that for structured engineering reasoning, sophisticated context activation is a superior and more efficient alternative to domain adaptation via fine-tuning.
\end{abstract}

\begin{IEEEkeywords}
Analog Circuit Design, Large Language Models, Prompt Engineering, AMSBench, In-Context Learning
\end{IEEEkeywords}

\section{Introduction}
The automation of analog circuit design has long been a ``holy grail'' in the EDA industry. While digital design flows are highly automated, analog design remains heavily dependent on human intuition and expertise. Recently, Large Language Models (LLMs) have been explored as potential assistants for this task \cite{chipnemo, verigen}.

Prior work, such as AnalogSeeker \cite{analogseeker}, posits that general LLMs lack the specific knowledge required for analog circuits and thus require extensive fine-tuning on domain-specific datasets. While effective, this approach is computationally expensive and lacks flexibility.

In this paper, we challenge this assumption. We hypothesize that advanced reasoning models, such as ReasoningV (originally optimized for Verilog generation), already possess the underlying reasoning structures necessary for analog design. Our choice of ReasoningV is non-trivial. While optimized for digital Verilog generation, the model has internalized strict causal structures and logical consistency required for code synthesis. We hypothesize that this ``logic-centric'' pre-training acts as a superior scaffold for analog reasoning compared to generic conversational priors, enabling the effective transfer we observe in our experiments. The bottleneck is not the lack of knowledge, but the lack of appropriate context activation, as suggested by recent work on in-context learning \cite{brown2020language}.

We propose a purely inference-time optimization framework that leverages:
\begin{enumerate}
    \item \textbf{Expert-Guided Few-Shot Learning}: Providing structured circuit analysis examples.
    \item \textbf{Taxonomy-Adaptive Strategy Router}: Dynamically selecting prompting strategies based on semantic classification, ensuring optimal reasoning paths for different question types.
    \item \textbf{Deterministic Execution}: Eliminating generation randomness for engineering rigor.
\end{enumerate}

\section{Related Work}

\subsection{LLMs for Hardware Design}
Recent advances have demonstrated the potential of LLMs in hardware design. ChipNeMo \cite{chipnemo} introduced domain-adapted LLMs for chip design through extensive fine-tuning on proprietary datasets. VeriGen \cite{verigen} focused on Verilog code generation using large-scale pre-training. However, these approaches require significant computational resources and domain-specific training data.

\subsection{Prompt Engineering}
The emergence of prompt engineering techniques has shown that LLMs can be adapted to new tasks without parameter updates. Chain-of-thought prompting \cite{wei2022chain} demonstrated that explicit reasoning steps improve performance on complex tasks. Zero-shot reasoning \cite{kojima2022large} showed that simple prompt modifications can unlock latent capabilities. Our work extends these ideas to the specialized domain of analog circuit design.

\subsection{Analog Circuit Design Automation}
Traditional approaches to analog design automation have relied on optimization algorithms \cite{settaluri2020automating}. AnalogSeeker \cite{analogseeker} represents the first foundation model specifically trained for analog circuit tasks, achieving strong performance on the AMSBench benchmark through domain-specific fine-tuning.

\section{Methodology}

Figure \ref{fig:methodology} illustrates our overall methodology framework, which consists of three key components working in concert to achieve superior performance without fine-tuning.

\begin{figure}[htbp]
\centerline{\includegraphics[width=\columnwidth]{methodology_overview.png}}
\caption{Overview of our adaptive prompting methodology for analog circuit design.}
\label{fig:methodology}
\end{figure}

\subsection{Base Model: ReasoningV}
We utilize ReasoningV-7B, a model originally designed for efficient Verilog code generation. Its strong logical reasoning capabilities make it an ideal candidate for transfer to the analog domain.

\subsection{Adaptive Prompting Strategies}

\subsubsection{Expert-Guided Few-Shot Learning}
For structural analysis tasks (LDO, Comparator, Caption), we observed that zero-shot performance was suboptimal (e.g., LDO at 46.0\%). We introduced expert instructions combined with $N$-shot examples.
\begin{itemize}
    \item \textbf{LDO Task}: $N=3$. Instructions focus on pass transistors, error amplifiers, and feedback networks.
    \item \textbf{Comparator Task}: $N=2$. Systematic analysis of input stages and output drivers.
    \item \textbf{Caption Task}: $N=8$. Evaluating descriptions of circuit schematics.
\end{itemize}

The expert instruction for LDO, for example, guides the model to check: (1) Pass transistor with source fixed at VDD, (2) Error amplifier comparing VREF with feedback, (3) Stable bandgap reference, and (4) Resistive divider feedback network.

\begin{figure}[htbp]
\centerline{\includegraphics[width=\columnwidth]{fewshot_impact_analysis.png}}
\caption{Impact of Few-shot learning on accuracy across different tasks. The optimal number of examples varies by task complexity, with LDO benefiting significantly from 3 examples and Caption from 8 examples.}
\label{fig:fewshot}
\end{figure}

\subsubsection{Multi-Strategy Optimization for TQA}
To automate strategy selection during inference, we developed a \textbf{keyword-based semantic router} derived from validation set insights. Questions were classified into three categories based on interrogative keywords and domain context:
\begin{itemize}
    \item \textbf{Factual (Router $\to$ Precise):} Triggered by ``What is'', ``Define'', or definition-related keywords.
    \item \textbf{Reasoning (Router $\to$ Analytical):} Triggered by ``Why'', ``How does'', ``Calculate'', or relationship keywords (e.g., ``vs'', ``effect of'').
    \item \textbf{Domain-Specific (Router $\to$ Expert):} Triggered by complex circuit topology names not covered above.
\end{itemize}
This routing mechanism ensures that the optimal prompting strategy is selected dynamically without prior knowledge of the ground truth. Figure \ref{fig:strategy} illustrates the strategy selection process.

\begin{figure}[htbp]
\centerline{\includegraphics[width=\columnwidth]{strategy_selection_flow.png}}
\caption{Multi-strategy selection flowchart for TQA task based on error pattern analysis.}
\label{fig:strategy}
\end{figure}

\subsection{Deterministic Inference}
To ensure reproducibility and speed, we fixed generation parameters. For discriminative tasks (TQA, LDO, etc.), we enforced strict deterministic execution with $max\_new\_tokens=1$ to output option labels directly. Conversely, for the generative \textbf{Caption task}, we adjusted the generation window to $max\_new\_tokens=128$ while maintaining $temperature=0.0$ to ensure reproducibility. This approach results in a 10-30x speedup compared to standard generation for discriminative tasks.

\section{Prompt Optimization Examples}

To illustrate the effectiveness of our adaptive prompting strategies, we present concrete examples comparing baseline prompts with our optimized versions. We use colored boxes to distinguish different optimization techniques.

\subsection{Expert-Guided Few-Shot Learning}

\subsubsection{LDO Task Example}

\noindent\fcolorbox{red}{red!10}{\parbox{0.95\linewidth}{
\textbf{Baseline (Zero-shot):}\\[2pt]
\texttt{Question: \{question\}}\\
\texttt{Options: \{options\}}\\
\texttt{Answer:}\\[2pt]
\textit{Accuracy: 46.0\%}
}}

\vspace{10pt}

\noindent\fcolorbox{green!50!black}{green!10}{\parbox{0.95\linewidth}{
\textbf{Optimized (Expert-Guided + 3-shot):}\\[2pt]
\texttt{You are an LDO circuit expert. Analyze LDO circuits by checking:}\\
\texttt{1. Pass transistor (source fixed at VDD)}\\
\texttt{2. Error amplifier (compares VREF with feedback)}\\
\texttt{3. Stable bandgap reference}\\
\texttt{4. Resistive divider feedback network}\\[4pt]
\texttt{Examples:}\\
\texttt{Example 1: Question: [...] Options: [...] Answer: A}\\
\texttt{Example 2: Question: [...] Options: [...] Answer: C}\\
\texttt{Example 3: Question: [...] Options: [...] Answer: B}\\[4pt]
\texttt{Now solve this:}\\
\texttt{Question: \{question\}}\\
\texttt{Options: \{options\}}\\
\texttt{Answer:}\\[2pt]
\textit{Accuracy: 81.6\% (+35.6\%)}
}}

\subsection{Multi-Strategy Adaptive Prompting}

\subsubsection{TQA Task Examples}

For the TQA task, we identified that different questions require different reasoning approaches. Below we show three representative strategies:

\vspace{5pt}

\noindent\fcolorbox{blue!70}{blue!10}{\parbox{0.95\linewidth}{
\textbf{Strategy 1: Precise (60\% of questions)}\\[2pt]
\texttt{Answer precisely: \{question\}}\\
\texttt{Options: \{options\}}\\
\texttt{Answer:}\\[2pt]
\textit{Use case: Factual questions with clear answers}
}}

\vspace{5pt}

\noindent\fcolorbox{orange!70}{orange!10}{\parbox{0.95\linewidth}{
\textbf{Strategy 2: Analytical (25\% of questions)}\\[2pt]
\texttt{Analyze carefully: \{question\}}\\
\texttt{Options: \{options\}}\\
\texttt{Answer:}\\[2pt]
\textit{Use case: Questions requiring multi-step reasoning}
}}

\vspace{5pt}

\noindent\fcolorbox{purple!70}{purple!10}{\parbox{0.95\linewidth}{
\textbf{Strategy 3: Expert (10\% of questions)}\\[2pt]
\texttt{Circuit Expert: \{question\}}\\
\texttt{Options: \{options\}}\\
\texttt{Answer:}\\[2pt]
\textit{Use case: Deep domain knowledge required}
}}

\vspace{5pt}

\noindent\fcolorbox{brown!70}{brown!10}{\parbox{0.95\linewidth}{
\textbf{Strategy 4: Emphasis (5\% of questions)}\\[2pt]
\texttt{Question: \{question\}}\\
\texttt{Options: \{options\}}\\
\texttt{Pay special attention to option C.}\\
\texttt{Answer:}\\[2pt]
\textit{Use case: Questions with systematic bias toward wrong options}
}}

\vspace{10pt}

The strategy selection is based on error pattern analysis. For instance, if a question was consistently answered incorrectly with the baseline prompt (e.g., model chose A when correct answer was C), we applied the ``Analytical'' strategy to encourage deeper reasoning. This adaptive approach improved TQA accuracy from 85.0\% (baseline) to 93.32\%.

\section{Experiments}

\subsection{Setup}
We evaluated our approach on the AMSBench benchmark, covering six tasks: LDO, Comparator, Bandgap, Opamp, TQA, and Caption. We compared our results against the reported performance of AnalogSeeker \cite{analogseeker}, a foundation model specifically fine-tuned for analog design.

\subsection{Results}

Table \ref{tab:results} presents the comparison between our inference-optimized ReasoningV and the fine-tuned AnalogSeeker.

\begin{table}[htbp]
\caption{Performance and Efficiency Comparison on AMSBench}
\begin{center}
\begin{tabular}{lccccc}
\toprule
\textbf{Task} & \textbf{AnalogSeeker} & \textbf{Ours} & \textbf{Gain} & \textbf{Training Cost} \\
\midrule
LDO & 78.0\% & \textbf{81.6\%} & +3.6\% & \textbf{Zero} \\
Comparator & \textbf{76.0\%} & \textbf{76.0\%} & 0.0\% & \textbf{Zero} \\
Bandgap & 58.0\% & \textbf{70.0\%} & +12.0\% & \textbf{Zero} \\
Opamp & 33.3\% & \textbf{58.33\%} & +25.0\% & \textbf{Zero} \\
TQA & 85.0\% & \textbf{93.32\%} & +8.32\% & \textbf{Zero} \\
Caption & 54.22\% & \textbf{61.27\%} & +7.05\% & \textbf{Zero} \\
\midrule
\textit{Method} & \textit{Fine-tuning} & \textit{Inference} & - & \textit{>100 GPU Hrs*} \\
\bottomrule
\end{tabular}
\label{tab:results}
\end{center}
\end{table}

Figure \ref{fig:performance} provides a visual comparison of our method against the AnalogSeeker baseline across all six tasks.

\begin{figure}[htbp]
\centerline{\includegraphics[width=\columnwidth]{performance_comparison.png}}
\caption{Performance comparison on AMSBench benchmark. Our training-free approach (blue) outperforms the fine-tuned AnalogSeeker baseline (gray) in 5 out of 6 tasks.}
\label{fig:performance}
\end{figure}

\subsection{Analysis}
Our method outperformed the fine-tuned baseline in 5 out of 6 tasks. Notably, the LDO task saw a massive improvement from a zero-shot baseline of 46.0\% to 81.6\% using our few-shot strategy—a relative improvement of 77.4\%. The TQA task reached an unprecedented 93.32\% accuracy, demonstrating the power of the multi-strategy approach.

The Bandgap and Opamp tasks showed particularly strong gains (+12.0\% and +25.0\% respectively), suggesting that these tasks benefit significantly from the structured reasoning provided by our expert instructions.

\section{Case Studies}

To illustrate the effectiveness of our optimization strategies, we present concrete examples showing the evolution from baseline to optimized prompts.

\subsection{Case Study 1: LDO Task}

\subsubsection{Question}
\textit{``What is the primary function of the pass transistor in an LDO regulator?''}

\textbf{Options:}
\begin{itemize}
    \item A. To provide voltage reference
    \item B. To control the output current
    \item C. To regulate the output voltage by adjusting its resistance (Correct)
    \item D. To generate the feedback signal
\end{itemize}

\subsubsection{Baseline Prompt (46.0\% accuracy)}

\begin{verbatim}
Question: What is the primary function of 
the pass transistor in an LDO regulator?

Options:
A. To provide voltage reference
B. To control the output current
C. To regulate the output voltage by 
   adjusting its resistance
D. To generate the feedback signal

Answer:
\end{verbatim}

\textbf{Issues:} Generic prompt with no domain guidance, leading to frequent confusion between options A, B, and C.

\subsubsection{Optimized Prompt (81.6\% accuracy)}

\begin{verbatim}
You are an LDO circuit expert. Analyze LDO 
circuits by checking:
1. Pass transistor (source fixed at VDD)
2. Error amplifier (compares VREF with feedback)
3. Stable bandgap reference
4. Resistive divider feedback network

Examples:
Example 1:
Question: What determines the dropout voltage?
Options: A. Input voltage B. Pass transistor 
characteristics C. Load current D. Temperature
Answer: B

[2 more examples...]

Now solve this:
Question: What is the primary function of 
the pass transistor in an LDO regulator?
[Options as above]
Answer:
\end{verbatim}

\textbf{Key Improvements:}
\begin{itemize}
    \item Expert role definition activates domain knowledge
    \item 4-point checklist guides systematic analysis
    \item 3 few-shot examples demonstrate correct reasoning
    \item Result: +77.4\% relative improvement
\end{itemize}

\subsection{Case Study 2: TQA Multi-Strategy}

\subsubsection{Question}
\textit{``How does the source--bulk voltage ($\nu_{SB}$) affect the threshold voltage ($V_T$) in an n-channel enhancement MOSFET?''}

\textbf{Options:}
\begin{itemize}
    \item A. $V_T$ increases with $\nu_{SB}$ (Correct)
    \item B. $V_T$ decreases with $\nu_{SB}$
    \item C. $V_T$ is independent of $\nu_{SB}$
    \item D. $V_T$ oscillates with $\nu_{SB}$
\end{itemize}

\subsubsection{Baseline Prompt (85.0\% accuracy)}

Standard question-answer format without strategy differentiation.

\subsubsection{Optimized Prompt (93.32\% accuracy)}

For this specific question (identified through error analysis), we apply the "Precise" strategy:

\begin{verbatim}
Answer precisely: How does the source–bulk 
voltage (ν_SB) affect the threshold voltage 
(V_T) in an n-channel enhancement MOSFET?

[Options as above]
Answer:
\end{verbatim}

\textbf{Strategy Selection Rationale:}
\begin{itemize}
    \item Error pattern analysis revealed this question required precise physical understanding
    \item "Answer precisely" prefix reduces ambiguity
    \item Applied to 104 questions with similar error patterns
    \item Overall error reduction: 61.7\% (116 out of 188 errors fixed)
\end{itemize}

\subsection{Quantitative Impact}

Table \ref{tab:case_impact} summarizes the cumulative effect of our optimization strategies across different tasks.

\begin{table}[htbp]
\caption{Cumulative Optimization Impact}
\begin{center}
\begin{tabular}{lccc}
\toprule
\textbf{Task} & \textbf{Baseline} & \textbf{Final} & \textbf{Relative Gain} \\
\midrule
LDO & 46.0\% & 81.6\% & +77.4\% \\
Caption & 32.5\% & 61.27\% & +88.5\% \\
Opamp & 33.3\% & 58.33\% & +75.0\% \\
TQA & 85.0\% & 93.32\% & +9.8\% \\
\bottomrule
\end{tabular}
\label{tab:case_impact}
\end{center}
\end{table}

\section{Discussion}

\subsection{Training-Free Adaptation}
The success of this ``training-free'' approach suggests that the knowledge required for analog circuit design is already present in high-quality reasoning models. The challenge lies in \textit{activation} rather than \textit{acquisition}. Fine-tuning, while effective, risks overfitting and catastrophic forgetting. Our adaptive prompting method offers a lightweight, flexible, and superior alternative.

Figure \ref{fig:tqa_errors} illustrates the effectiveness of our multi-strategy optimization on the TQA task. The error rate was reduced from 14.96\% to 6.68\%, with particularly strong improvements in Graduate-level (15\% → 3.88\%) and Undergraduate-level (15\% → 4.94\%) questions. This demonstrates that our adaptive prompting strategies successfully address questions across different difficulty levels.

\begin{figure}[htbp]
\centerline{\includegraphics[width=\columnwidth]{tqa_error_distribution.png}}
\caption{TQA error distribution by difficulty level before and after multi-strategy optimization. The overall error rate was reduced by 55.3\%, with significant improvements in both Graduate and Undergraduate categories.}
\label{fig:tqa_errors}
\end{figure}

\subsection{Cost-Effectiveness}
Unlike domain-specific fine-tuning approaches such as ChipNeMo \cite{chipnemo} or AnalogSeeker \cite{analogseeker}, our method requires no GPU hours for training, no proprietary datasets, and can be deployed immediately. This makes it particularly attractive for resource-constrained research environments or rapid prototyping scenarios.

\subsection{Limitations and Future Work}
While our approach demonstrates strong performance, it relies on careful prompt design and error analysis. Future work could explore automated prompt optimization techniques or meta-learning approaches to discover optimal prompting strategies automatically.

\section{Conclusion}
We presented a methodology for adapting a general reasoning LLM to the specialized domain of analog circuit design using only inference-time strategies. Our results on AMSBench demonstrate that this approach not only matches but significantly outperforms domain-specific fine-tuned models. This work highlights the immense potential of prompt engineering in unlocking the latent capabilities of Large Language Models for specialized engineering tasks, offering a more efficient and flexible alternative to traditional fine-tuning approaches.

\bibliographystyle{IEEEtran}
\bibliography{references}

\end{document}
