% --- LLMs for EDA ---

@article{chipnemo,
  title={ChipNeMo: Domain-Adapted LLMs for Chip Design},
  author={Liu, Mingjie and Elliot, Kexin and Ai, Wenlong and Bansal, Sachit and Yue, Yuhan and Liu, Xin and others},
  journal={arXiv preprint arXiv:2311.00176},
  year={2023}
}

@inproceedings{verigen,
  title={VeriGen: A Large Language Model for Verilog Code Generation},
  author={Thakur, Shailja and Tsai, Tsung-Wei and Ahmad, Maryam and Liao, Yibo and others},
  booktitle={ACM/IEEE International Symposium on Quality Electronic Design (ISQED)},
  year={2024}
}

% --- The Baseline ---
@article{analogseeker,
  title={AnalogSeeker: An Open-source Foundation Language Model for Analog Circuit Design},
  author={Chen, Zihao and Zhuang, Ji and Shen, Jinyi and Ke, Xiaoyue and Yang, Xinyi and Zhou, Mingjie and Du, Zhuoyao and Yan, Xu and Wu, Zhouyang and Xu, Zhenyu and Huang, Jiangli and Shang, Li and Zeng, Xuan and Yang, Fan},
  journal={arXiv preprint arXiv:2508.10409},
  year={2025}
}

% --- Prompt Engineering Foundations ---

@inproceedings{wei2022chain,
  title={Chain-of-Thought Prompting Elicits Reasoning in Large Language Models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le, Quoc and Zhou, Denny},
  booktitle={Advances in Neural Information Processing Systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}

@article{brown2020language,
  title={Language Models are Few-Shot Learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{kojima2022large,
  title={Large Language Models are Zero-Shot Reasoners},
  author={Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={22199--22213},
  year={2022}
}

% --- Analog Design Context ---

@inproceedings{settaluri2020automating,
  title={Automating Analog Constraint Extraction: From Heuristics to Learning},
  author={Settaluri, Kourosh and Hakhamaneshi, Kourosh and Klinefelter, Aaron and Blalock, Davis and Gonzalez, Joseph and Nikolic, Borivoje and Abbeel, Pieter and Stoica, Vladimir},
  booktitle={Design, Automation \& Test in Europe Conference \& Exhibition (DATE)},
  pages={1--6},
  year={2020},
  organization={IEEE}
}

@misc{anthropic2025frontend,
  title={Improving frontend design through Skills},
  author={Anthropic},
  year={2025},
  month={November},
  howpublished={\url{https://www.claude.com/blog/improving-frontend-design-through-skills}},
  note={Accessed: 2025-11-25}
}

@article{deepmind_test_time,
  title={Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters},
  author={Snell, Charlie and others},
  journal={arXiv preprint arXiv:2408.03314},
  year={2024}
}

@inproceedings{react,
  title={ReAct: Synergizing Reasoning and Acting in Language Models},
  author={Yao, Shunyu and Zhao, Jeffrey and Yu, Dian and Du, Nan and Shafran, Izhak and Narasimhan, Karthik and Cao, Yuan},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2023}
}

@article{medpalm,
  title={Large Language Models Encode Clinical Knowledge},
  author={Singhal, Karan and Azizi, Shekoofeh and Tu, Tao and Mahdavi, S Sara and Wei, Jason and Chung, Hyung Won and Scales, Nathan and Tanwani, Ajay and Cole-Lewis, Heather and Pfohl, Stephen and others},
  journal={Nature},
  volume={620},
  number={7972},
  pages={172--180},
  year={2023},
  publisher={Nature Publishing Group}
}
