#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¶ˆèå®éªŒè„šæœ¬ (Ablation Study)
é€æ­¥éªŒè¯æ¯ç§ä¼˜åŒ–ç­–ç•¥çš„ç‹¬ç«‹æ•ˆæœ
"""

import json
import torch
import time
import os
from transformers import AutoTokenizer, AutoModelForCausalLM
from typing import Dict, List, Any, Optional
import warnings
warnings.filterwarnings("ignore")


class AblationStudy:
    """æ¶ˆèå®éªŒç±»"""
    
    def __init__(self, model_path: str):
        """åˆå§‹åŒ–"""
        self.model_path = model_path
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.model = None
        self.tokenizer = None
        
        # å®éªŒé…ç½®
        self.experiments = {
            "baseline": {
                "name": "åŸºçº¿é…ç½®",
                "prompt_template": "Question: {question}\n\nOptions:\n{options}\n\nAnswer:",
                "params": {
                    "max_new_tokens": 3,
                    "temperature": 0.1,
                    "do_sample": True,
                    "top_p": 0.9,
                    "top_k": 10
                }
            },
            "params_only": {
                "name": "ä»…å‚æ•°ä¼˜åŒ–",
                "prompt_template": "Question: {question}\n\nOptions:\n{options}\n\nAnswer:",
                "params": {
                    "max_new_tokens": 1,
                    "temperature": 0.0,
                    "do_sample": False,
                    "top_p": 1.0,
                    "top_k": 1,
                    "use_cache": True
                }
            },
            "prompt_only": {
                "name": "ä»…æç¤ºè¯ä¼˜åŒ–",
                "prompt_template": "Circuit Expert: {question}\n\nOptions:\n{options}\n\nAnswer:",
                "params": {
                    "max_new_tokens": 3,
                    "temperature": 0.1,
                    "do_sample": True,
                    "top_p": 0.9,
                    "top_k": 10
                }
            },
            "few_shot_1": {
                "name": "Few-shot 1ç¤ºä¾‹",
                "use_few_shot": True,
                "num_examples": 1,
                "expert_instruction": "You are a circuit expert.",
                "params": {
                    "max_new_tokens": 1,
                    "temperature": 0.0,
                    "do_sample": False,
                    "top_p": 1.0,
                    "top_k": 1,
                    "use_cache": True
                }
            },
            "few_shot_2": {
                "name": "Few-shot 2ç¤ºä¾‹",
                "use_few_shot": True,
                "num_examples": 2,
                "expert_instruction": "You are a circuit expert.",
                "params": {
                    "max_new_tokens": 1,
                    "temperature": 0.0,
                    "do_sample": False,
                    "top_p": 1.0,
                    "top_k": 1,
                    "use_cache": True
                }
            },
            "few_shot_3": {
                "name": "Few-shot 3ç¤ºä¾‹",
                "use_few_shot": True,
                "num_examples": 3,
                "expert_instruction": "You are a circuit expert.",
                "params": {
                    "max_new_tokens": 1,
                    "temperature": 0.0,
                    "do_sample": False,
                    "top_p": 1.0,
                    "top_k": 1,
                    "use_cache": True
                }
            },
            "full_optimization": {
                "name": "å®Œæ•´ä¼˜åŒ–ï¼ˆå‚æ•°+æç¤ºè¯+Few-shotï¼‰",
                "use_few_shot": True,
                "num_examples": 3,
                "expert_instruction": "You are an LDO circuit expert. Analyze LDO circuits by checking:\n1. Pass transistor (source fixed at VDD)\n2. Error amplifier (compares VREF with feedback)\n3. Stable bandgap reference\n4. Resistive divider feedback network\n",
                "params": {
                    "max_new_tokens": 1,
                    "temperature": 0.0,
                    "do_sample": False,
                    "top_p": 1.0,
                    "top_k": 1,
                    "use_cache": True
                }
            }
        }
    
    def load_model(self):
        """åŠ è½½æ¨¡å‹"""
        print(f"ğŸ“¥ æ­£åœ¨åŠ è½½æ¨¡å‹: {self.model_path}")
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_path, trust_remote_code=True)
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_path,
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True
        )
        print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
    
    def generate_answer(self, prompt: str, params: Dict) -> str:
        """ç”Ÿæˆç­”æ¡ˆ"""
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device)
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                **params
            )
        
        answer = self.tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True).strip()
        return answer[0] if answer else "A"
    
    def create_few_shot_prompt(self, question: str, options: str, num_examples: int, expert_instruction: str, examples: List[Dict]) -> str:
        """åˆ›å»ºFew-shotæç¤ºè¯"""
        prompt = expert_instruction + "\n\n"
        prompt += "Examples:\n"
        
        for i, example in enumerate(examples[:num_examples], 1):
            prompt += f"Example {i}:\n"
            prompt += f"Question: {example['question']}\n"
            prompt += f"Options:\n{example['options']}\n"
            prompt += f"Answer: {example['answer']}\n\n"
        
        prompt += "Now solve this:\n"
        prompt += f"Question: {question}\n"
        prompt += f"Options:\n{options}\n"
        prompt += "Answer:"
        
        return prompt
    
    def test_task(self, task_data: List[Dict], experiment_name: str, task_name: str = "LDO") -> Dict:
        """æµ‹è¯•å•ä¸ªä»»åŠ¡"""
        config = self.experiments[experiment_name]
        correct = 0
        total = len(task_data)
        times = []
        
        # å‡†å¤‡Few-shotç¤ºä¾‹ï¼ˆå¦‚æœæœ‰ï¼‰
        examples = []
        if config.get("use_few_shot", False) and task_name == "LDO":
            # LDOä»»åŠ¡çš„Few-shotç¤ºä¾‹
            examples = [
                {
                    "question": "What determines the dropout voltage in an LDO?",
                    "options": "A. Input voltage level\nB. Pass transistor characteristics\nC. Load current\nD. Temperature",
                    "answer": "B"
                },
                {
                    "question": "How does the error amplifier work in an LDO?",
                    "options": "A. It compares input and output\nB. It compares reference and feedback\nC. It amplifies the load current\nD. It generates the reference voltage",
                    "answer": "B"
                },
                {
                    "question": "What is the role of the feedback network in an LDO?",
                    "options": "A. To sense the input voltage\nB. To divide the output voltage\nC. To control the pass transistor\nD. To generate the reference",
                    "answer": "B"
                }
            ]
        
        print(f"\nğŸ§ª å®éªŒ: {config['name']}")
        print(f"   æµ‹è¯•é¢˜ç›®æ•°: {total}")
        
        for i, item in enumerate(task_data[:10]):  # åªæµ‹è¯•å‰10é¢˜ä»¥èŠ‚çœæ—¶é—´
            question = item.get("question", "")
            options = item.get("options", "")
            groundtruth = item.get("ground_truth", item.get("groundtruth", "A"))
            
            # æ„å»ºæç¤ºè¯
            if config.get("use_few_shot", False):
                prompt = self.create_few_shot_prompt(
                    question, 
                    options, 
                    config.get("num_examples", 1),
                    config.get("expert_instruction", ""),
                    examples
                )
            else:
                prompt = config["prompt_template"].format(question=question, options=options)
            
            # ç”Ÿæˆç­”æ¡ˆ
            start_time = time.time()
            try:
                answer = self.generate_answer(prompt, config["params"])
                if answer.upper() == groundtruth.upper():
                    correct += 1
            except Exception as e:
                print(f"   é”™è¯¯ (é¢˜ç›® {i+1}): {e}")
                answer = "A"
            
            times.append(time.time() - start_time)
            
            if (i + 1) % 5 == 0:
                print(f"   è¿›åº¦: {i+1}/{min(10, total)}")
        
        accuracy = (correct / min(10, total)) * 100 if total > 0 else 0
        avg_time = sum(times) / len(times) if times else 0
        
        return {
            "experiment": experiment_name,
            "name": config["name"],
            "correct": correct,
            "total": min(10, total),
            "accuracy": accuracy,
            "avg_time": avg_time
        }
    
    def run_ablation_study(self, task_data: List[Dict], task_name: str = "LDO") -> Dict:
        """è¿è¡Œæ¶ˆèå®éªŒ"""
        print("=" * 80)
        print(f"æ¶ˆèå®éªŒ: {task_name} ä»»åŠ¡")
        print("=" * 80)
        
        results = {}
        
        # æŒ‰é¡ºåºè¿è¡Œæ¯ä¸ªå®éªŒ
        experiment_order = [
            "baseline",
            "params_only",
            "prompt_only",
            "few_shot_1",
            "few_shot_2",
            "few_shot_3",
            "full_optimization"
        ]
        
        for exp_name in experiment_order:
            result = self.test_task(task_data, exp_name, task_name)
            results[exp_name] = result
        
        return results
    
    def save_results(self, results: Dict, output_file: str):
        """ä¿å­˜ç»“æœ"""
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        print(f"\nâœ… ç»“æœå·²ä¿å­˜åˆ°: {output_file}")


def main():
    """ä¸»å‡½æ•°"""
    import sys
    
    if len(sys.argv) < 2:
        print("ç”¨æ³•: python ablation_study.py <model_path> [task_data_file]")
        sys.exit(1)
    
    model_path = sys.argv[1]
    task_data_file = sys.argv[2] if len(sys.argv) > 2 else None
    
    # åˆ›å»ºæ¶ˆèå®éªŒå¯¹è±¡
    study = AblationStudy(model_path)
    study.load_model()
    
    # åŠ è½½ä»»åŠ¡æ•°æ®ï¼ˆç¤ºä¾‹ï¼‰
    if task_data_file and os.path.exists(task_data_file):
        with open(task_data_file, 'r', encoding='utf-8') as f:
            task_data = json.load(f)
    else:
        # ä½¿ç”¨ç¤ºä¾‹æ•°æ®
        task_data = [
            {
                "question": "What is the primary function of the pass transistor in an LDO regulator?",
                "options": "A. To provide voltage reference\nB. To control the output current\nC. To regulate the output voltage by adjusting its resistance\nD. To generate the feedback signal",
                "ground_truth": "C"
            }
        ] * 10
    
    # è¿è¡Œæ¶ˆèå®éªŒ
    results = study.run_ablation_study(task_data, "LDO")
    
    # ä¿å­˜ç»“æœ
    output_file = "results/ablation_study_results.json"
    os.makedirs("results", exist_ok=True)
    study.save_results(results, output_file)
    
    # æ‰“å°æ€»ç»“
    print("\n" + "=" * 80)
    print("æ¶ˆèå®éªŒç»“æœæ€»ç»“")
    print("=" * 80)
    for exp_name, result in results.items():
        print(f"{result['name']:30s} | å‡†ç¡®ç‡: {result['accuracy']:6.2f}% | å¹³å‡æ—¶é—´: {result['avg_time']:.3f}s")


if __name__ == "__main__":
    main()
